{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML malware detector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCcAjJ1r8khOthFIA0g+q9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincedu/ML-vs-DL-malware-detection/blob/main/ML_malware_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a static malware detector**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l389lunGfDRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary package\n",
        "\n",
        "> `pefile` package is used to parse the PE header of an executable file and extract the multitude of values of the PE header.\n",
        "\n"
      ],
      "metadata": {
        "id": "RXzyZ_90uujV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQacOQpNuwdn"
      },
      "outputs": [],
      "source": [
        "pip install sklearn nltk pefile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enumerate the samples and assigning their labels"
      ],
      "metadata": {
        "id": "QlpWHUC3Q4yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import listdir\n",
        "\n",
        "directories_with_labels = [(\"Benign PE Samples\", 0), (\"Malicious PE Samples\", 1)]\n",
        "list_of_samples = []\n",
        "labels = []\n",
        "for dataset_path, label in directories_with_labels:\n",
        "    samples = [f for f in listdir(dataset_path)]\n",
        "    for sample in samples:\n",
        "      ext = os.path.splitext(sample)[-1].lower()\n",
        "      if ext == '.exe':\n",
        "        file_path = os.path.join(dataset_path, sample)\n",
        "        list_of_samples.append(file_path)\n",
        "        labels.append(label)\n",
        "print(list_of_samples)\n",
        "print(len(list_of_samples))\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "C-jeiXUGyW2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform a stratified train-test split"
      ],
      "metadata": {
        "id": "ljlMAX95RB4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "samples_train, samples_test, labels_train, labels_test = train_test_split(\n",
        "    list_of_samples, labels, test_size=0.3, stratify=labels, random_state=11\n",
        ")\n",
        "print(len(samples_train))\n",
        "print(samples_train)\n",
        "print(labels_train)\n",
        "print(len(samples_test))\n",
        "print(samples_test)\n",
        "print(labels_test)"
      ],
      "metadata": {
        "id": "R-a-ZcFaQWqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "DwSDi5MHEwAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from nltk import ngrams\n",
        "import numpy as np\n",
        "import pefile\n",
        "\n",
        "\n",
        "def read_file(file_path):\n",
        "    \"\"\"Reads in the binary sequence of a binary file.\"\"\"\n",
        "    with open(file_path, \"rb\") as binary_file:\n",
        "        data = binary_file.read()\n",
        "    return data\n",
        "\n",
        "\n",
        "def byte_sequence_to_Ngrams(byte_sequence, N):\n",
        "    \"\"\"Creates a list of N-grams from a byte sequence.\"\"\"\n",
        "    Ngrams = ngrams(byte_sequence, N)\n",
        "    return list(Ngrams)\n",
        "\n",
        "\n",
        "def binary_file_to_Ngram_counts(file, N):\n",
        "    \"\"\"Takes a binary file and outputs the N-grams counts of its binary sequence.\"\"\"\n",
        "    filebyte_sequence = read_file(file)\n",
        "    file_Ngrams = byte_sequence_to_Ngrams(filebyte_sequence, N)\n",
        "    return collections.Counter(file_Ngrams)\n",
        "\n",
        "\n",
        "def get_NGram_features_from_sample(sample, K1_most_frequent_Ngrams_list):\n",
        "    \"\"\"Takes a sample and produces a feature vector.\n",
        "    The features are the counts of the K1 N-grams we've selected.\n",
        "    \"\"\"\n",
        "    K1 = len(K1_most_frequent_Ngrams_list)\n",
        "    feature_vector = K1 * [0]\n",
        "    file_Ngrams = binary_file_to_Ngram_counts(sample, N)\n",
        "    for i in range(K1):\n",
        "        feature_vector[i] = file_Ngrams[K1_most_frequent_Ngrams_list[i]]\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def preprocess_imports(list_of_DLLs):\n",
        "    \"\"\"Normalize the naming of the imports of a PE file.\"\"\"\n",
        "    temp = [x.decode().split(\".\")[0].lower() for x in list_of_DLLs]\n",
        "    return \" \".join(temp)\n",
        "\n",
        "\n",
        "def get_imports(pe):\n",
        "    \"\"\"Get a list of the imports of a PE file.\"\"\"\n",
        "    list_of_imports = []\n",
        "    for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
        "        list_of_imports.append(entry.dll)\n",
        "    return preprocess_imports(list_of_imports)\n",
        "\n",
        "def get_section_names(pe):\n",
        "    \"\"\"Gets a list of section names from a PE file.\"\"\"\n",
        "    list_of_section_names = []\n",
        "    for sec in pe.sections:\n",
        "        normalized_name = sec.Name.decode().replace(\"\\x00\", \"\").lower()\n",
        "        list_of_section_names.append(normalized_name)\n",
        "    return \"\".join(list_of_section_names)"
      ],
      "metadata": {
        "id": "mN7qCkt6EvA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select the 100 most frequent 2-grams\n",
        "\n",
        "> Background: In the literature and industry, it has been determined that the most frequent N-grams are also the most informative ones for a malware classification algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "xZlvHMSIG4gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 2\n",
        "Ngram_counts_all = collections.Counter([])\n",
        "for sample in samples_train:\n",
        "    Ngram_counts_all += binary_file_to_Ngram_counts(sample, N)\n",
        "K1 = 100\n",
        "K1_most_frequent_Ngrams = Ngram_counts_all.most_common(K1)\n",
        "K1_most_frequent_Ngrams_list = [x[0] for x in K1_most_frequent_Ngrams]\n",
        "\n",
        "print(K1_most_frequent_Ngrams_list)"
      ],
      "metadata": {
        "id": "TTgM0CsHG1Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract features\n",
        "\n",
        "\n",
        "> Extract the N-gram counts, section names, imports, and number of sections of each sample in our training test, and skip over samples whose PE header cannot be parsed\n"
      ],
      "metadata": {
        "id": "6LhJ2jiyvY-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imports_corpus_train = []\n",
        "num_sections_train = []\n",
        "section_names_train = []\n",
        "Ngram_features_list_train = []\n",
        "y_train = []\n",
        "for i in range(len(samples_train)):\n",
        "    sample = samples_train[i]\n",
        "    try:\n",
        "        NGram_features = get_NGram_features_from_sample(\n",
        "            sample, K1_most_frequent_Ngrams_list\n",
        "        )\n",
        "        pe = pefile.PE(sample)\n",
        "        imports = get_imports(pe)\n",
        "        n_sections = len(pe.sections)\n",
        "        sec_names = get_section_names(pe)\n",
        "        imports_corpus_train.append(imports)\n",
        "        num_sections_train.append(n_sections)\n",
        "        section_names_train.append(sec_names)\n",
        "        Ngram_features_list_train.append(NGram_features)\n",
        "        y_train.append(labels_train[i])\n",
        "    except Exception as e:\n",
        "        print(sample + \":\")\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "m1vRpNRxvYTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vectorize PE header features\n",
        "\n",
        "\n",
        "> hashing vectorizer followed by `tfidf` to convert the imports and section names, both of which are text features, into a numerical form using a basic NLP approach\n",
        "\n"
      ],
      "metadata": {
        "id": "Vk9_4GqDv7k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "imports_featurizer = Pipeline(\n",
        "    [\n",
        "        (\"vect\", HashingVectorizer(input=\"content\", ngram_range=(1, 2))),\n",
        "        (\"tfidf\", TfidfTransformer(use_idf=True,)),\n",
        "    ]\n",
        ")\n",
        "section_names_featurizer = Pipeline(\n",
        "    [\n",
        "        (\"vect\", HashingVectorizer(input=\"content\", ngram_range=(1, 2))),\n",
        "        (\"tfidf\", TfidfTransformer(use_idf=True,)),\n",
        "    ]\n",
        ")\n",
        "imports_corpus_train_transformed = imports_featurizer.fit_transform(\n",
        "    imports_corpus_train\n",
        ")\n",
        "section_names_train_transformed = section_names_featurizer.fit_transform(\n",
        "    section_names_train\n",
        ")"
      ],
      "metadata": {
        "id": "teC2fGy5v6sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine the vectorized features into a single array\n",
        "\n",
        "\n",
        "> by using the `scipy` hstack to merge different features into one large sparse `scipy` array\n",
        "\n"
      ],
      "metadata": {
        "id": "SGDSHFK3knak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "X_train = hstack(\n",
        "    [\n",
        "        Ngram_features_list_train,\n",
        "        imports_corpus_train_transformed,\n",
        "        section_names_train_transformed,\n",
        "        csr_matrix(num_sections_train).transpose(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(X_train)"
      ],
      "metadata": {
        "id": "9rjs-uqzknz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a Random Forest classifier on the training set\n",
        "with default parameters"
      ],
      "metadata": {
        "id": "_1JMwnfvkv6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "_aUueW55kvU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collect the features of the testing set, just as we did for the training set"
      ],
      "metadata": {
        "id": "3GdE7kwRk_dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imports_corpus_test = []\n",
        "num_sections_test = []\n",
        "section_names_test = []\n",
        "Ngram_features_list_test = []\n",
        "y_test = []\n",
        "for i in range(len(samples_test)):\n",
        "    file = samples_test[i]\n",
        "    try:\n",
        "        NGram_features = get_NGram_features_from_sample(\n",
        "            sample, K1_most_frequent_Ngrams_list\n",
        "        )\n",
        "        pe = pefile.PE(file)\n",
        "        imports = get_imports(pe)\n",
        "        n_sections = len(pe.sections)\n",
        "        sec_names = get_section_names(pe)\n",
        "        imports_corpus_test.append(imports)\n",
        "        num_sections_test.append(n_sections)\n",
        "        section_names_test.append(sec_names)\n",
        "        Ngram_features_list_test.append(NGram_features)\n",
        "        y_test.append(labels_test[i])\n",
        "    except Exception as e:\n",
        "        print(sample + \":\")\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "aFhQR3vIlGG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine the vectorized features into a single array\n",
        "\n",
        "\n",
        "> by applying the previously trained transformers to vectorize the text features"
      ],
      "metadata": {
        "id": "f8L-_MoMb5vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imports_corpus_test_transformed = imports_featurizer.transform(imports_corpus_test)\n",
        "section_names_test_transformed = section_names_featurizer.transform(section_names_test)\n",
        "X_test = hstack(\n",
        "    [\n",
        "        Ngram_features_list_test,\n",
        "        imports_corpus_test_transformed,\n",
        "        section_names_test_transformed,\n",
        "        csr_matrix(num_sections_test).transpose(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(X_test)"
      ],
      "metadata": {
        "id": "Iq8ZFcjvb7do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Random Forest classifier on the resulting test set"
      ],
      "metadata": {
        "id": "HHYDyGwRlJkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"accuracy: \",accuracy_score(y_test, y_pred))\n",
        "print(\"precision: \",precision_score(y_test, y_pred))\n",
        "print(\"recall: \",recall_score(y_test, y_pred))\n",
        "print(\"f1 score: \",f1_score(y_test, y_pred))\n",
        "print(\"roc: \",roc_auc_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Yl_bZgY7lUJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a Naive Bayes classifier on the training set"
      ],
      "metadata": {
        "id": "XH7g1H6ralWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "bnb = BernoulliNB(alpha=0.3)\n",
        "bnb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "j0l4eKSyalsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test our classifier on the resulting test set"
      ],
      "metadata": {
        "id": "WtjjSHXfc752"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "print(\"accuracy: \",accuracy_score(y_test, y_pred))\n",
        "print(\"precision: \",precision_score(y_test, y_pred))\n",
        "print(\"recall: \",recall_score(y_test, y_pred))\n",
        "print(\"f1 score: \",f1_score(y_test, y_pred))\n",
        "print(\"roc: \",roc_auc_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "JwIPYuZXc7Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a Support Vector Machines (SVM) classifier on the training set"
      ],
      "metadata": {
        "id": "sj-uE8xdeprx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "svc = svm.SVC()\n",
        "svc.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Y-1BjI9oepRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test our classifier on the resulting test set"
      ],
      "metadata": {
        "id": "N-jjtIXJfGZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "print(\"accuracy: \",accuracy_score(y_test, y_pred))\n",
        "print(\"precision: \",precision_score(y_test, y_pred))\n",
        "print(\"recall: \",recall_score(y_test, y_pred))\n",
        "print(\"f1 score: \",f1_score(y_test, y_pred))\n",
        "print(\"roc: \",roc_auc_score(y_test, y_pred))\n",
        "\n",
        "y_pred = svc.predict(X_train)\n",
        "\n",
        "print(\"accuracy: \",accuracy_score(y_train, y_pred))\n",
        "print(\"precision: \",precision_score(y_train, y_pred))\n",
        "print(\"recall: \",recall_score(y_train, y_pred))\n",
        "print(\"f1 score: \",f1_score(y_train, y_pred))\n",
        "print(\"roc: \",roc_auc_score(y_train, y_pred))"
      ],
      "metadata": {
        "id": "irVVce9JfHVL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}